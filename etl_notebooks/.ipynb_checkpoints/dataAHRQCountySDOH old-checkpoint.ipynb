{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f32e62-545f-4334-ad58-83358ab613fe",
   "metadata": {},
   "source": [
    "# Data ETL, Agency for Healthcare Research and Quality SDOH\n",
    "Extract and clean AHRQ survey data. Save a dictionary to be used for LangChain SQLDatabase tool.  \n",
    "Provide unction out_AHRQCountySDOH() to return dataframe. See https://www.ahrq.gov/sdoh/data-analytics/sdoh-data.html and https://www.ahrq.gov/sites/default/files/wysiwyg/sdoh/SDOH-Data-Sources-Documentation-v1-Final.pdf\n",
    "\n",
    "Input parameters:\n",
    "- parm_AHRQCountySDOH_years: list of survey years\n",
    "- parm_AHRQCountySDOH_surveys: list of surveys to extract\n",
    "- parm_AHRQCountySDOH_questions: CDCW and CHR have some SDOH questions which could be used (add these)\n",
    "\n",
    "Typical surveys to use include:\n",
    "\n",
    "[\"ACS\", \"AHA\", \"AMFAR\", \"CCBP\", \"CDCSVI\", \"CEN\", \"CRDC\", \"EPAA\", \"FARA\", \"FEA\", \"HHC\", \"HIFLD\", \"HRSA\", \"MHSVI\", \"MP\", \"NCHS\", \"NEPHTN\", \"NHC\", \"NOAAC\", \"NOAAS\", \"POS\", \"SAHIE\", \"SAIPE\", \"SEDA\"]\n",
    "\n",
    "These surveys are health outcomes and would not be used as SDOH:\n",
    "\n",
    "    AHRF, CDCA, CDCAP, CDCP, CDCW, CHR and MGV\n",
    "\n",
    "    \n",
    "However, CDCW and CHR have some SDOH questions which could be used in the list of questions:\n",
    "\n",
    "[\"CDCW_INJURY_DTH_RATE\", \"CDCW_TRANSPORT_DTH_RATE\", \"CDCW_SELFHARM_DTH_RATE\", \"CDCW_ASSAULT_DTH_RATE\", \"CHR_TOT_MENTAL_PROV\", \"CHR_MENTAL_PROV_RATE\", \"CHR_SEGREG_BLACK\", \"CHR_PCT_ALCOHOL_DRIV_DEATH\", \"CHR_PCT_EXCESS_DRINK\", \"CHR_PCT_FOOD\", \"CHR_SEGREG_BLACK\", \"CHR_SEGREG_NON_WHITE\"]\n",
    "\n",
    "Note 2018 has some behavioral health questions which could be used:\n",
    "\n",
    "[\"CDCP_NO_PHY_ACTV_ADULT_A\", \"CDCP_NO_PHY_ACTV_ADULT_C\", \"CDCP_SLEEP_LESS7HR_ADULT_A\", \"CDCP_SLEEP_LESS7HR_ADULT_C\"]\n",
    "    \n",
    "CAF are County Adjacent FIPS codes which could be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa81f8ea-0172-4673-af7d-11c42918c611",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import requests\n",
    "import re\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "# for imports from agents\n",
    "sys.path.append('../agents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbcc663-7351-4d84-ada5-b46955fdbd9e",
   "metadata": {},
   "source": [
    "### Parms\n",
    "From calling Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6bb6949-0052-425f-933c-570dc6c3fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parm_AHRQCountySDOH_years = ['2015', '2016']\n",
    "# parm_AHRQCountySDOH_surveys = [\"ACS\", \"AHA\", \"AMFAR\", \"CAF\", \"CCBP\", \"CDCSVI\", \"CEN\", \"CRDC\", \"EPAA\", \"FARA\", \"FEA\", \"HHC\", \"HIFLD\", \"HRSA\", \"MHSVI\", \"MP\", \"NCHS\", \"NEPHTN\", \"NHC\", \"NOAAS\", \"POS\", \"SAHIE\", \"SAIPE\", \"SEDA\"]\n",
    "# parm_AHRQCountySDOH_questions = [\"CDCW_INJURY_DTH_RATE\", \"CDCW_TRANSPORT_DTH_RATE\", \"CDCW_SELFHARM_DTH_RATE\", \"CDCW_ASSAULT_DTH_RATE\", \"CHR_TOT_MENTAL_PROV\", \"CHR_MENTAL_PROV_RATE\", \"CHR_SEGREG_BLACK\", \"CHR_PCT_ALCOHOL_DRIV_DEATH\", \"CHR_PCT_EXCESS_DRINK\", \"CHR_PCT_FOOD\", \"CHR_SEGREG_BLACK\", \"CHR_SEGREG_NON_WHITE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67772fd9-a166-43d3-a1fa-a0aa7f28d060",
   "metadata": {},
   "source": [
    "### Download Method\n",
    "If we are running in a container, download can get CloudFront 403 meaning the site is blocking “non-browser” clients. Fix it by sending browser-like headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aebef40b-1611-47e9-b8f1-60ecbf90fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_excel_with_browser_headers(url: str, out_path: str = None, session: requests.Session = None, timeout: int = 30) -> bytes:\n",
    "    headers = {\n",
    "        \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                       \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                       \"Chrome/120.0.0.0 Safari/537.36\"),\n",
    "        \"Accept\": \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n",
    "        # \"Referer\": \"https://www.ahrq.gov/sdoh/index.html\",\n",
    "    }\n",
    "    s = session or requests.Session()\n",
    "    r = s.get(url, headers=headers, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    content = r.content\n",
    "    if out_path:\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb037498-c78b-47cc-9152-f263c0b38b42",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "883bb635-9ee5-40cc-86f4-b20a5eacdb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AHRQCountySDOH_year = parm_AHRQCountySDOH_years.pop(0)\n",
    "# url = 'https://www.ahrq.gov/sites/default/files/wysiwyg/sdoh/SDOH_'+  AHRQCountySDOH_year +'_COUNTY_1_0.xlsx'\n",
    "# out_path = download_excel_with_browser_headers(url, \"./ahrq\"+ AHRQCountySDOH_year +\".xlsx\")\n",
    "# dfAHRQCountySDOH = pd.read_excel(out_path, sheet_name=\"Data\", engine=\"openpyxl\")\n",
    "# dfAHRQCountySDOH = dfAHRQCountySDOH.drop(dfAHRQCountySDOH.columns[[ 0,1,2,5,6 ]],axis = 1)\n",
    "# dfAHRQCountySDOH['YEAR'] =  AHRQCountySDOH_year\n",
    "\n",
    "# # Import the remaining excel files and append to dataframe\n",
    "# for AHRQCountySDOH_year in parm_AHRQCountySDOH_years:\n",
    "#     out_path = download_excel_with_browser_headers(url, \"./ahrq\"+ AHRQCountySDOH_year +\".xlsx\")\n",
    "#     dfAHRQCountySDOHnext = pd.read_excel(out_path, sheet_name=\"Data\", engine=\"openpyxl\")\n",
    "#     dfAHRQCountySDOHnext = dfAHRQCountySDOHnext.drop(dfAHRQCountySDOHnext.columns[[ 0,1,2,5,6 ]],axis = 1)\n",
    "#     dfAHRQCountySDOHnext['YEAR'] =  AHRQCountySDOH_year\n",
    "#     dfAHRQCountySDOH = pd.concat([dfAHRQCountySDOH, dfAHRQCountySDOHnext], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "400a8741-dabc-4764-9bd6-de4036d099d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parm_AHRQCountySDOH_years' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m failed_years = []\n\u001b[32m      3\u001b[39m session = requests.Session()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m yr \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparm_AHRQCountySDOH_years\u001b[49m:\n\u001b[32m      6\u001b[39m     url = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://www.ahrq.gov/sites/default/files/wysiwyg/sdoh/SDOH_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_COUNTY_1_0.xlsx\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      7\u001b[39m     out_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./ahrq\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.xlsx\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'parm_AHRQCountySDOH_years' is not defined"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "failed_years = []\n",
    "session = requests.Session()\n",
    "\n",
    "for yr in parm_AHRQCountySDOH_years:\n",
    "    url = f'https://www.ahrq.gov/sites/default/files/wysiwyg/sdoh/SDOH_{yr}_COUNTY_1_0.xlsx'\n",
    "    out_path = f\"./ahrq{yr}.xlsx\"\n",
    "    try:\n",
    "        # small retry loop for transient failures\n",
    "        attempts = 0\n",
    "        while True:\n",
    "            attempts += 1\n",
    "            try:\n",
    "                excel_bytes = download_excel_with_browser_headers(url, out_path=out_path, session=session)\n",
    "                break\n",
    "            except requests.HTTPError as e:\n",
    "                if attempts >= 3:\n",
    "                    raise\n",
    "                time.sleep(1 * attempts)\n",
    "        # read directly from bytes (avoids needing to rely on disk if you want)\n",
    "        df = pd.read_excel(pd.io.common.BytesIO(excel_bytes), sheet_name=\"Data\", engine=\"openpyxl\")\n",
    "        # replicate your column-dropping and YEAR assignment\n",
    "        df = df.drop(df.columns[[0, 1, 2, 5, 6]], axis=1)\n",
    "        df['YEAR'] = yr\n",
    "        df_list.append(df)\n",
    "        print(f\"Loaded year {yr} ({len(df)} rows).\")\n",
    "    except Exception as e:\n",
    "        failed_years.append((yr, str(e)))\n",
    "        print(f\"Failed to load year {yr}: {e}\")\n",
    "\n",
    "# concatenate all successfully loaded years\n",
    "if df_list:\n",
    "    dfAHRQCountySDOH = pd.concat(df_list, ignore_index=True)\n",
    "else:\n",
    "    dfAHRQCountySDOH = pd.DataFrame()  # empty fallback\n",
    "\n",
    "if failed_years:\n",
    "    print(\"Some years failed to load:\", failed_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b50d1d5-1cf8-4fe5-9532-70149201feee",
   "metadata": {},
   "source": [
    "### Clean data\n",
    "Convert to numeric, forcing non-convertible values to NaN and remove county from names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95368a20-b534-43f5-9cf9-9c157695b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAHRQCountySDOH.iloc[:, 3:] = dfAHRQCountySDOH.iloc[:, 3:].apply(pd.to_numeric, errors='coerce')\n",
    "dfAHRQCountySDOH['COUNTY'] = dfAHRQCountySDOH['COUNTY'].str.replace(' County','')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cc78b7-21e8-4911-b1a3-a32c54f61c47",
   "metadata": {},
   "source": [
    "### Filter to SDOH surverys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b05f493-651e-4787-a511-916bb51d8421",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAHRQCountySDOHred = dfAHRQCountySDOH[[\"STATE\", \"COUNTY\", \"YEAR\"]] \n",
    "\n",
    "dfAHRQCountySDOHsel = dfAHRQCountySDOH[dfAHRQCountySDOH.columns[pd.Series(dfAHRQCountySDOH.columns).str.startswith(tuple(parm_AHRQCountySDOH_surveys))]]\n",
    "dfAHRQCountySDOHred2 = pd.concat([dfAHRQCountySDOHred, dfAHRQCountySDOHsel], axis=1)\n",
    "\n",
    "dfAHRQCountySDOHsel = dfAHRQCountySDOH[parm_AHRQCountySDOH_questions]\n",
    "dfAHRQCountySDOHred = pd.concat([dfAHRQCountySDOHred2, dfAHRQCountySDOHsel], axis=1)\n",
    "dfAHRQCountySDOHnew = dfAHRQCountySDOHred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a574baee-8aa8-4b2e-9bc5-8eb02c88160b",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "Commented out, keep all columns for now.\n",
    "\n",
    "Remove columns missing more than 30%.  Then impute missing data using KNN imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b9ee2-5002-43a8-a89b-ad6c70fe09e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop columns that are more than 30% null\n",
    "# dfAHRQCountySDOHnew = dfAHRQCountySDOHnew.dropna(axis = 1, thresh=len(dfAHRQCountySDOHnew)*.7)\n",
    "\n",
    "# from sklearn.impute import KNNImputer\n",
    "# imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "# dfAHRQCountySDOHnew.iloc[:,3::] = imputer.fit_transform(dfAHRQCountySDOHnew.iloc[:,3::])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce408973-863a-4a61-aa84-75c78fee2df2",
   "metadata": {},
   "source": [
    "### Method to get SDOH Code Book and build dictionary of column descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa4318-6d13-48a1-9ecc-10c57ba68d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sdoh_codebook_as_custom_table_info(\n",
    "    codebook_url: str,\n",
    "    table_name: str,\n",
    "    table_description: str = \"AHRQ SDOH dataset\",\n",
    "    out_path: str = \"./ahrq_codebook.xlsx\"\n",
    ") -> dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    codebook_url : str\n",
    "        HTTPS URL to the Codebook file.\n",
    "    table_name : str\n",
    "        Exact table name as it exists in the database.\n",
    "    table_description : str, optional\n",
    "        Description of the table contents.\n",
    "    out_path : str, optional\n",
    "        Local file path to save the codebook.\n",
    "\n",
    "    Returns\n",
    "    dict[str, dict]\n",
    "        custom_table_info structure, e.g.\n",
    "        {\n",
    "            \"table_name\": {\n",
    "                \"table_description\": \"...\",\n",
    "                \"columns\": {\n",
    "                    \"col1\": \"Human description\",\n",
    "                    \"col2\": \"Human description\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Download codebook using your helper (return file path)\n",
    "    file_path = download_excel_with_browser_headers(\n",
    "        codebook_url, out_path=out_path\n",
    "    )\n",
    "\n",
    "    # Load all sheets\n",
    "    all_sheets = pd.read_excel(file_path, sheet_name=None, engine=\"openpyxl\")\n",
    "\n",
    "    # Pick sheet containing \"all variables\"\n",
    "    sheet_name = next(\n",
    "        (name for name in all_sheets if \"all\" in name.lower() and \"variable\" in name.lower()),\n",
    "        list(all_sheets.keys())[0]\n",
    "    )\n",
    "    df = all_sheets[sheet_name]\n",
    "\n",
    "    # Locate relevant columns\n",
    "    var_col = next(\n",
    "        (c for c in df.columns if re.search(r'\\b(var(iable)?(\\s*name)?|short\\s*name)\\b', str(c), re.I)),\n",
    "        None\n",
    "    )\n",
    "    label_col = next(\n",
    "        (c for c in df.columns if re.search(r'\\b(label|description|long\\s*name|title)\\b', str(c), re.I)),\n",
    "        None\n",
    "    )\n",
    "\n",
    "    if var_col is None or label_col is None:\n",
    "        raise ValueError(\n",
    "            f\"Could not identify variable/label columns in sheet '{sheet_name}'. \"\n",
    "            f\"Columns seen: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Build columns dictionary\n",
    "    columns = {\n",
    "        str(row[var_col]).strip(): str(row[label_col]).strip()\n",
    "        for _, row in df.iterrows()\n",
    "        if pd.notna(row[var_col]) and pd.notna(row[label_col])\n",
    "    }\n",
    "\n",
    "    # Wrap in custom_table_info format\n",
    "    custom_table_info = {\n",
    "        table_name: {\n",
    "            \"table_description\": table_description,\n",
    "            \"columns\": columns\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return custom_table_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ef97e-f502-45ff-a877-644837ab2bf5",
   "metadata": {},
   "source": [
    "### Method to build vector store from column descriptors dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a35318-91ee-405b-aade-fca4f8c73a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any\n",
    "from pathlib import Path\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def build_schema_index(\n",
    "    full_mapping: Dict[str, dict],        # { table: {\"table_description\": str, \"columns\": {col: desc, ...}} }\n",
    "    embeddings,\n",
    "    persist_dir: str = \"../data/schema_faiss_index\"                  \n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Build/persist a Chroma vector index for schema chunks.\n",
    "\n",
    "    Each chunk's metadata will contain:\n",
    "      - table (str)\n",
    "      - column (str)\n",
    "      - text (short human text, e.g. \"county_fips: 5-digit FIPS code\")\n",
    "\n",
    "    This version intentionally omits column 'type' and per-table summary chunks,\n",
    "    because the SQL tool/inspector provides authoritative types and you want\n",
    "    minimal, compact chunks for retrieval.\n",
    "    \"\"\"\n",
    "    docs: List[str] = []\n",
    "    metadatas: List[Dict[str, Any]] = []\n",
    "\n",
    "    for table, tinfo in full_mapping.items():\n",
    "        if isinstance(tinfo, dict):\n",
    "            cols = tinfo.get(\"columns\", {}) or {}\n",
    "        else:\n",
    "            cols = {}\n",
    "\n",
    "        # index one chunk per column (minimal)\n",
    "        for col_name, col_desc in cols.items():\n",
    "            col_desc_str = str(col_desc).strip() if col_desc is not None else \"\"\n",
    "            text = f\"{col_name}: {col_desc_str}\" if col_desc_str else f\"{col_name}\"\n",
    "            # store the human text as document content and in metadata\n",
    "            docs.append(text)\n",
    "            metadatas.append({\n",
    "                \"table\": table,\n",
    "                \"column\": col_name,\n",
    "                \"text\": text,\n",
    "            })\n",
    "\n",
    "    # save the FAISS index and metadata to the directory\n",
    "    persist_path = Path(persist_dir)\n",
    "    persist_path.mkdir(parents=True, exist_ok=True)\n",
    "    vectordb = FAISS.from_texts(texts=docs, embedding=embeddings, metadatas=metadatas)\n",
    "    vectordb.save_local(str(persist_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7626080f",
   "metadata": {},
   "source": [
    "## Method to normalize column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066efad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_colname(name: str) -> str:\n",
    "    \"\"\"\n",
    "    1) lower-case\n",
    "    2) replace any non-alphanumeric character with underscore\n",
    "    3) collapse multiple underscores\n",
    "    4) strip leading/trailing underscores\n",
    "    5) if starts with digit -> prefix with 'col_'\n",
    "    6) special-case a 'year' column -> rename to 'year_col' to avoid reserved-word collisions\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        return name\n",
    "    s = str(name).lower()\n",
    "    # replace non-alphanumeric with underscore\n",
    "    s = re.sub(r'[^a-z0-9]', '_', s)\n",
    "    # collapse multiple underscores\n",
    "    s = re.sub(r'_+', '_', s)\n",
    "    s = s.strip('_')\n",
    "    # if empty after cleaning\n",
    "    if not s:\n",
    "        s = 'col'\n",
    "    # prefix if starts with digit\n",
    "    if re.match(r'^[0-9]', s):\n",
    "        s = 'col_' + s\n",
    "    # avoid a bare \"year\" column name that can sometimes be problematic\n",
    "    if s == 'year':\n",
    "        s = 'sdoh_year'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110aa61",
   "metadata": {},
   "source": [
    "### Method to normalize names in the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1345602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_mapping_columns(mapping: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Normalize all column names in the mapping using normalize_colname().\n",
    "    \"\"\"\n",
    "    normalized_mapping = {}\n",
    "    for table, tinfo in mapping.items():\n",
    "        normalized_cols = {normalize_colname(col): desc for col, desc in tinfo.get(\"columns\", {}).items()}\n",
    "        normalized_mapping[table] = {\n",
    "            \"table_description\": tinfo.get(\"table_description\", \"\"),\n",
    "            \"columns\": normalized_cols\n",
    "        }\n",
    "    return normalized_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a4f880",
   "metadata": {},
   "source": [
    "*** Method to remove duplicate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c9a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_columns(mapping: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Remove duplicate column names in the mapping after normalization.\n",
    "    Keeps the first occurrence of a column name, drops duplicates.\n",
    "    \"\"\"\n",
    "    cleaned_mapping = {}\n",
    "    for table, tinfo in mapping.items():\n",
    "        seen = set()\n",
    "        cleaned_cols = {}\n",
    "        for col, desc in tinfo.get(\"columns\", {}).items():\n",
    "            if col not in seen:\n",
    "                cleaned_cols[col] = desc\n",
    "                seen.add(col)\n",
    "            else:\n",
    "                # optionally log dropped columns\n",
    "                print(f\"Dropped duplicate column '{col}' in table '{table}'\")\n",
    "        cleaned_mapping[table] = {\n",
    "            \"table_description\": tinfo.get(\"table_description\", \"\"),\n",
    "            \"columns\": cleaned_cols\n",
    "        }\n",
    "    return cleaned_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08bbdcc-8444-42c1-81ea-43ad3172376c",
   "metadata": {},
   "source": [
    "### Get column descriptors, normalize columns, remove duplicates and persist in vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d05094b-b0c5-48fb-b465-e48ebb7cce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformer import SentenceTransformerWrapper\n",
    "\n",
    "mapping = load_sdoh_codebook_as_custom_table_info(\n",
    "    \"https://www.ahrq.gov/sites/default/files/wysiwyg/sdoh/SDOH_2020_Codebook_1_0.xlsx\",\n",
    "    \"sdoh_surveys\",\n",
    "    \"AHRQ SDOH Surveys\",\n",
    "    \"./ahrq_codebook.xlsx\"\n",
    ")\n",
    "\n",
    "mapping = normalize_mapping_columns(mapping)\n",
    "\n",
    "mapping = remove_duplicate_columns(mapping)\n",
    "\n",
    "emb = SentenceTransformerWrapper(model_name=\"all-MiniLM-L6-v2\")\n",
    "build_schema_index(\n",
    "    mapping,\n",
    "    emb,\n",
    "    \"../data\",\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57db6acd-e546-49d6-aa39-4da84895a1eb",
   "metadata": {},
   "source": [
    "### Load function\n",
    "Function for calling notebook to get dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1926a73-cbc9-4bcd-a5ca-42adad8ae45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove formatting issues by converting YEAR to int\n",
    "dfAHRQCountySDOHnew[\"YEAR\"] = pd.to_numeric(dfAHRQCountySDOHnew[\"YEAR\"])\n",
    "def out_AHRQCountySDOH():\n",
    "   return dfAHRQCountySDOHnew"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
